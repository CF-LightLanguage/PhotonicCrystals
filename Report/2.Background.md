2.1. Physical fundaments
Photonic crystal (PhC) is the integration of solid-state physics and electromagnetism. The following chapter focuses on comprehending these two fields of physics, investigating how the wave propagation through PhCs occurs and why certain frequency range of light are prohibited from propagating inside the crystal structure, a phenomenon known as the photonic bandgap.

2.1.1. Photonic crystals
“PhC can be defined as composites with a space-periodical internal structure operating at frequencies where the wavelength is comparable to the period of the structure. (...) Such regular composites are designed to operate at optical frequencies (from the far infrared range to the visible and ultraviolet light)”2.
Frequency bands in which the electromagnetic waves cannot propagate through it are referred to as bandgaps. The constituents of the crystal and the geometry of the lattice define these bandgaps3. Geometrically, there are three classes of PhCs represented below, each distinguished by its complexity of alternating material and dimensionality.
Figure 1. One- two-, and three-dimensional photonic crystals [1, pg. 4]
This study focuses on the analysis and simulation of two-dimensional PhCs, consisting of a structure composed of two alternated materials with different dielectric constants.

  
2.1.2. Electromagnetism in in Mixed Dielectric Media
The study of the propagation of light in a photonic crystal is governed by the four macroscopic Maxwell equations4, described as:
Considering the propagation within a mixed dielectric medium, in which the structure does not vary with time, and there are no free charges or currents (! = 0 and @ = 0), further approximations are assumed:
1. linear regime, so that all higher-order terms ( A&'() are neglected.
2. the material is macroscopic and isotropic, so that
=(#) = "!"(#):(#) and 5(#) = #!#(#)>(#)
3. frequency dependence of the dielectric constant is ignored, since the value of
dielectric constant is chosen appropriate to the considered frequency range.
4. only transparent materials, so that "# is real and positive.
With these, the Maxwell equations become:
To write a harmonic mode as a spatial pattern, the physical field can be decoupled from the complex-valued field, employing the complex exponential:
Converting the Maxwell equations once again into the following:
∇. B"(#):(#)C = 0 ∇ × >(#) + EF"!"(#) :(#) = 0
3
  ∇. 5 = 0
 ∇ × : + <5 = 0 <%
    ∇. = = !
 ∇ × > − <= = @ <%
    ∇.>(#,*) =0
∇×: +# <>(#,*)=0 (#,*) ! <%
     ∇. B"(#):(#,*)C = 0
 ∇ × > − " " <:(#,*) = 0 (#,*) !(#) <%
     >(#,*) = >(#)D+&,*
   :(#,*) = :(#)D+&,*
   ∇.>(#) =0
  ∇ × :(#) − EF#! >(#) = 0
 4 Cf. Joannopoulos, 2022, S. 6.

With an adequate replacement to eliminate :(#), the master equation is set: ∇ × G 1 ∇×>(#)I=JFL->(#)
"(#) K
After >(#) is found, the electrical field can be calculated with the following equation: :(#) = E ∇×>(#)
An eigenvalue problem is identified in the master equation: the Hermitian operator
'.'
Θ = ∇ × M/(") ∇ ×N. That means, the operator Θ operates on an eigenmode >(#) results
in an eigenvalue (F⁄K)- times the eigenmode:
4
 F"!"(#)
F- Θ>(#) =JKL >(#)
2.1.3. Solid-states physics applied to 2D photonic crystals
A periodically arrangement of units is a simplified theoretical definition of crystalline solid. The unit themselves may be single atoms, groups of atoms, molecules, ions, etc. bravais lattice is the name given to the geometry of the periodic structure in which this repeated units of the crystal are arranged and a two-dimensional (2D) consists all points with position vector ( of the form ( = P.Q. + P-Q- , where P. QPR P- ∈ Z , Q. QPR Q- are the position vectors. An area of the space that fills all of space without either overlapping itself or leaving void is called a primitive unit cell of the lattice. 5 6
A further relevant definition leads to a better analytical understanding of 2D periodic structures and its interaction with light: considering a set of points ( and a plane wave D&(.#, the set of all wave vectors k that yield plane waves with the periodicity of a given bravais lattice is known as its reciprocal lattice. It can be characterized as the set of wave vectors k satisfying D&(.1 = 1 for all ( in the Bravais lattice. And finally, the primitive cell of a reciprocal lattice is known as the first Brillouin zone.
5 Cf. Ibach, Lüth, 2009, pg. 21-23.
6 Cf. Ashcroft, Mermim, 1976, pg. 63-89
'
 
Because the ions in a perfect crystal are arranged in a regular periodic array, it is considered the problem of an electron in a potential )(+) with the periodicity of the underlying bravais lattice, for all Bravais lattice vectors (: 7
)(+ + () = )(+)
A typical crystalline potential might be expected to have the form shown in the figure X below
Figure 2. Typical crystalline periodic potential, plotted along a line of ions and along a line midway between a plane of ions. (closed cycles are the equilibrium ion sites; the solid curves give the potential along the line of ions; the dotted curves give the potential along a line between planes of ions; the dashed curves gibe the potential of single isolated ions.) [8, pg. 133]
Independent electrons, each of which obeys a one electron Schrödinger equation >.=G−ħ$ ∇-+)(+)I.=".withaperiodicpotential,areknownasBlochelectrons.
-3
The stationary states of the Bloch electron as a consequence of the periodicity of the potential U leads to the Bloch’s Theorem:
.4((+) = D&(.#W4((+)
Where n is the band Index, k (wave vector) is from the first brillouin zone and )(+ + () = )(+) for all ( in a bravais lattice, it can be chosen to have the form of a plane wave times a function with the periodicity of the Bravais lattice. 8
The solution for the master equation ∇ × M . ∇ × >(#)N = J,L- >(#) is found by /(") 5
analogy with the Bloch’s Theorem:
>((+) = D&(.#W((+)
5
  7 Ashcroft, Mermim, 1976, pg. 132 8 Ashcroft, Mermim, 1976, pg. 133

2.1.4. Origin of photonic band gaps
The key phenomenon inherent to the PhCs is the so-called Bragg phenomenon or Bragg’s scattering. A result of the in-phase interference of waves, reflected from a set of parallel crystal planes. As observed in the Figure 3, in the uniform space between any two crystal planes, the propagating field of a lattice eigenmode can be considered as a sum of two waves traveling in opposite directions: one with the amplitude A propagating along x with the wavevector X = XY!, and the other one propagating in the opposite direction with the amplitude AR, where R is the reflection coefficient at the reference plane x. At any point x, this “reflected” wave is a sum of partial waves reflected by all crystal planes located behind x. 9
Figure 3. Bragg phenomenon [2, pg. 97]
The result for R comprises a geometric progression with a factor DYZ(−[2XQ)
] = +D+'(6 + +D+'(6D+'-(7 + +D+'(6D+'8(7 + ⋯ = +D+'(6 1
1 − D+'-(7
considering that the path of the wave reflected from an arbitrary crystal plane is larger by 2Q than the path of the wave reflected from the previous crystal plane, implying the phase shift 2XQ for two waves reflected by adjacent crystal planes.
If DYZ(−[2XQ) = 1 and X ≡ F⁄` = ab⁄Q, where a is an integer number and ` is the speed of light in the host medium, the right-hand side of the equation diverges (] → ∞), this means that the propagation of wave along x at frequencies F4 = a` b⁄Q is impossible.
6
   9 Cf. Simovski, Tretyakov, 2020, pg. 96-97.

7 Another condition to be imposed, due to the fact that fine values of |]| larger than unity
are also prohibited, is |]| < 1, obtaining an inequality describing the passbands: 10 |Kgh2XQ| < 1 − |+|-
At these frequencies, the propagation along x is allowed. The graphic solution bellow shows these frequencies and the periodically alternated lattice passbands and bandgaps.
Figure 4. Concept of bandgaps in a set of parallel equidistant crystal planes [2, pg.98]
Bandgaps distinguishes in complete and directional bandgaps. In a complete bandgap, the propagation is forbidden in all directions and for all polarizations of waves, as showed in the figure 5. If the propagation is forbidden for some directions and allowed for the other ones the corresponding frequency band is called directional bandgap, represented in the figure 6 for transversal magnetic (TM) and electric modes (TE). 11
2
  Figure 5. Example of photonic band structure for a triangular array of air colums drilled in a dialectric substrate [1, pg. 76]
10 Cf. Simovski, Tretyakov, 2020, pg. 97-98. 11 Simovski, Tretyakov, 2020, pg. 100
 
Figure 6. Examples of photonic band structures for a squared array of dielectric columns and for the lowest-frequency modes of a square array of dielectric veins in air [1, pg. 68-72]
2.2. Machine learning techniques
Supervised learning concept is the type of ML, where it is assumed that all of examples extracted from the dataset, that has been previously created, called training examples, are labeled12. The goal is for the model to learn from the labeled examples so that it can be accurately predict the output labels for new, previously unseen inputs. A fundamental practice in supervised learning is to split the dataset into three subsets:
- Training set: the model learns from these data during training process by adjusting its parameters (weights and biases) to minimize the loss function.
- Validation set: tunes hyperparameters and evaluates the performance of the
model during training.
- Test set: it represents unseen data, examples that the model has not seen
during training or validation. With these, it evaluates the real-world performance
2.2.1. Deep Convolutional Neural Network
The convolutional neural network (CNN) is the class of deep learning system commonly used in supervised learning tasks, which helps to process the images. This network uses the translation invariance, which means the ability to recognize patterns or features regardless of their position withing the input data, and shaped weight characteristics. Consequently, this network is referred to as space invariant artificial
8
   12 Jo, 2021, pg. 3

neural network. This CNN helps to find the objects, scenes, faces, and patterns effectively. During the pattern recognition process, the network learns the features from the image, with the help of these patterns, the incoming images are classified successfully also eliminating the manual feature extraction process.13
This technique is useful to the fact that: it eliminates the manual feature extraction process instead of that, features are learned from the input image; CNN produces maximum recognition results compared to other classifiers; with the help of the pre- existing network information, new input details are retrained and successfully recognize the information.14
The PyTorch library15, an open-source deep learning framework was employed in this simulation. Various modules to operate the feature extraction and classification are mathematically described:
1. Feature extraction: The first convolutional layer might catch simple patterns like
-
lines and edges. Layers deeper in the network combine these simple patterns to recognize more complex shapes and eventually whole objects: 16
nn.Conv2d: applies a 2D convolution over an input signal composed of several input planes. The mathematical operator merges two sets of information: the convolutions are performed on the input data with the use of a filter or kernel to produce a feature map. By sliding the filter over the input and performing dot products between the filters and local regions of the input.
Figure 7: Input and filter: on the left, the filter slides over the input. On the right, the result is summed and added to the feature map [10]
;%&+.
gW%(i&,j9:*')=kEQh(j9:*')+ l mDEnh%(j9:*',X)⋆EPZW%(i&,X) (<!
⋆: cross-correlation operator17
9
  13 Cf. Shanthini, Manogaran, Vadinu, 2023, pg. 37 14 Cf. Shanthini, Manogaran, Vadinu, 2023, pg. 37 15 Source: https://pytorch.org
16 Source [17]
17 In signal processing, cross-correlation measures the similarity between a vector x and shifted copies of a vector y as a function of the lag. Source: https://de.mathworks.com

N: batch size
C: number of channels
Bias: unique scalar value which is added to the output of a convolutional at a
very single pixel
H: height of input planes in pixels
W: width in pixels
Weight: is the filter for the j-th output channel and k-th input channel Input: is the input tensor
- nn.BatchNorm2D: applies Batch Normalization for 2D input data. For each channel in the input data, it normalizes the activations by subtracting the mean and dividing by the standard deviation. The normalized activations are then scaled by a learnable parameter (gamma) and shifted by another learnable parameter (beta). These parameters are learned during training and allow the network to adapt and control the normalization process, updated through backpropagation18. This function speeds the training process, reduces overfitting and reduce the sensitivity of the network to the initial weights.
p= Y−:(Y) ∗u+v qrQ+(Y) + s
- nn.ReLu activation function applies the rectified linear unit function element-wise, to introduce non-linearity into the network, allowing it to learn complex patterns and representations from the input data. It returns x for positive values and zero for negative input values:
]DwW(Y) = (Y)= = aQY(0, Y)
Figure 8: ReLu operation [11]
18 A method of adjusting the output of a multi-layered neural network to produce a desired state for a given input (...), adjusting the connection weights to reduce the discrepancy between required and current outputs. Source: https://www.oxfordreference.com
10
    
- nn.Maxpool2d: applies a 2D max pooling over an input signal composed of several input planes. It reduces the spatial dimensions of the input while retaining the most important information, makes the network faster, reduce computational cost and prevents overfitting.
Figure 9: Max Pooling operation [10]
- Flatten: align all the pieces in a single row so they can be analyzed sequentially. The multi-dimensional array of features is converted into a one-dimensional array. By flattening the feature maps, the data is prepared for the last stage of processing, where the network combines all the learned features to make a classification (e.g., identifying the image as a cat, dog, etc.).
2. Classification: dense layers work by taking all the inputs from the previous flattened layer and determining the patterns that most strongly suggest a particular classification. Each neuron in a dense layer is connected to every input. These connections allow the neurons to weigh the inputs based on their importance for making accurate predictions, essentially voting on what the image represents. The power of dense layers comes from their ability to learn these weights through training, by adjusting these weights, the network gets better at making predictions.
- nn.Linear: applies a linear transformation to the incoming data, with 2% the weight matrix corresponding to the set of weights connecting one input feature to all output features and Bias b a vector added to the linear transformation output. Both parameters are learned though optimization algorithms to adjust such that the model’s predictions match the ground truth labels and be able to perform the classification.
11
  Q = Y2% + k

12
  Input layer
Output layer
 Figure 10: example of nn.Linear operator by a simple neural network [12]
3. Training the NN: The parameters determine how the model learns, via learning rates and optimizers, how much it learns at a time, given by the batch size (dataset is divided into smaller batches to accelerate the learning process) and how long it keeps learning, given by the number of epochs, which represent one full cycle through the entire training dataset.19 It must ensure that the model learns efficiently and effectively without overfitting or underfitting, by iteratively adjusting the network parameters to minimize the discrepancy between predicted and actual outputs.
